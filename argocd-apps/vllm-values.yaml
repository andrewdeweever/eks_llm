# Default values for vLLM Helm chart.
# This file contains recommended settings for deploying vLLM on Kubernetes with GPU support.
# Assumptions:
# - Helm chart from https://helm.vllm.ai, chart name: vllm, version: 0.1.0
# - GPU nodes labeled with 'type: g'
# - AWS EKS with ALB Ingress Controller installed
# - Model downloaded from HuggingFace; for private models, create secret 'vllm-secrets' with 'hf-token'

replicaCount: 1

image:
  repository: ghcr.io/vllm-project/vllm-openai
  tag: "0.5.1"
  pullPolicy: IfNotPresent

serviceAccount:
  create: true
  annotations: {}

service:
  type: ClusterIP
  port: 80
  targetPort: 8000

ingress:
  enabled: true
  ingressClassName: alb
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}]'
    alb.ingress.kubernetes.io/actions.ssl-redirect: '{"Type": "redirect", "RedirectConfig": {"Protocol": "HTTPS", "Port": "443", "StatusCode": "HTTP_301"}}'
  hosts:
    - host: vllm.example.com  # Change to your domain
      paths:
        - path: /
          pathType: Prefix
  tls: []

resources:
  requests:
    cpu: 2
    memory: 8Gi
    nvidia.com/gpu: 1
  limits:
    cpu: 4
    memory: 16Gi
    nvidia.com/gpu: 1

nodeSelector:
  type: g

tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

affinity: {}

vllmConfig:
  model: meta-llama/Llama-2-7b-chat-hf  # Default model, customize as needed
  tensor_parallel_size: 1
  max_model_len: 4096
  gpu_memory_utilization: 0.9
  # Additional vLLM args can be added here if the chart supports

extraEnv: []
# For secrets management, add to extraEnv or chart specific (e.g., HF_TOKEN from secret)

persistence:
  enabled: false  # Enable if you want persistent storage for models
  storageClass: ""
  accessMode: ReadWriteOnce
  size: 100Gi

podSecurityContext:
  fsGroup: 1000

securityContext:
  allowPrivilegeEscalation: false