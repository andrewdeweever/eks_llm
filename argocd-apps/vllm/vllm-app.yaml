apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: vllm
  namespace: argocd  # Assumes ArgoCD is installed in 'argocd' namespace; adjust if different
  # Add labels or annotations if needed, e.g., finalizers: [resources-finalizer.argocd.argoproj.io]
spec:
  project: default  # Assumes default ArgoCD project; create a dedicated project for better isolation
  source:
    repoURL: https://github.com/vllm-project/vllm.git  # Official vLLM Git repo
    targetRevision: main  # Use a specific tag/release for production, e.g., v0.5.1
    path: examples/online_serving/chart-helm  # Path to the Helm chart in the repo; ArgoCD will detect and use it as Helm source
    helm:
      releaseName: vllm  # Helm release name
      values: |
        replicaCount: 1

        image:
          repository: ghcr.io/vllm-project/vllm-openai
          tag: "0.5.1"
          pullPolicy: IfNotPresent

        serviceAccount:
          create: true
          annotations: {}

        service:
          type: ClusterIP
          port: 80
          targetPort: 8000

        ingress:
          enabled: true
          className: nginx  # Use nginx-ingress-controller
          annotations:
            cert-manager.io/cluster-issuer: letsencrypt-prod  # Assumes ClusterIssuer 'letsencrypt-prod' from cert-manager; adjust if different
            nginx.ingress.kubernetes.io/ssl-redirect: "true"  # Redirect HTTP to HTTPS
            # Additional nginx annotations if needed, e.g., nginx.ingress.kubernetes.io/proxy-body-size: "100m" for large responses
          hosts:
            - host: vllm.deweever.bsisandbox.com  # Change to your domain
              paths:
                - path: /
                  pathType: Prefix
          tls:
            - secretName: vllm-tls  # Auto-generated by cert-manager
              hosts:
                - vllm.deweever.bsisandbox.com

        resources:
          requests:
            cpu: 2
            memory: 8Gi
            nvidia.com/gpu: 1
          limits:
            cpu: 4
            memory: 16Gi
            nvidia.com/gpu: 1

        nodeSelector:
          type: g

        tolerations:
          - key: nvidia.com/gpu
            operator: Exists
            effect: NoSchedule

        affinity: {}

        vllmConfig:
          model: meta-llama/Llama-2-7b-chat-hf  # Default model, customize as needed
          tensor_parallel_size: 1
          max_model_len: 4096
          gpu_memory_utilization: 0.9
          # Additional vLLM args can be added here if the chart supports

        extraEnv: []
        # For secrets management, add to extraEnv or chart specific (e.g., HF_TOKEN from secret)

        persistence:
          enabled: false  # Enable if you want persistent storage for models
          storageClass: ""
          accessMode: ReadWriteOnce
          size: 100Gi

        podSecurityContext:
          fsGroup: 1000

        securityContext:
          allowPrivilegeEscalation: false      
  destination:
    server: https://kubernetes.default.svc  # In-cluster Kubernetes API
    namespace: vllm  # Target namespace for deployment
  syncPolicy:
    automated:
      prune: true  # Automatically delete resources not in Git
      selfHeal: true  # Automatically sync if out of sync
    syncOptions:
      - CreateNamespace=true  # Create the 'vllm' namespace if it doesn't exist
      - ApplyOutOfSyncOnly=true  # Optional: Only apply changes if out of sync
    # Sync waves can be defined in the Helm chart hooks if needed; not specified here
  # IgnoreDifferences can be added for resources that change externally, e.g., for GPU resources
  # info: {}  # Optional: Add application info like URL after sync