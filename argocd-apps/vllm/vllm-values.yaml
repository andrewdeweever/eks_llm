# Default values for vLLM Helm chart.
# This file contains recommended settings for deploying vLLM on Kubernetes with GPU support.
# Assumptions:
# - Helm chart from https://helm.vllm.ai, chart name: vllm, version: 0.1.0
# - GPU nodes labeled with 'type: g'
# - AWS EKS with ALB Ingress Controller installed
# - Model downloaded from HuggingFace; for private models, create secret 'vllm-secrets' with 'hf-token'

replicaCount: 1

image:
  repository: ghcr.io/vllm-project/vllm-openai
  tag: "0.5.1"
  pullPolicy: IfNotPresent

serviceAccount:
  create: true
  annotations: {}

service:
  type: ClusterIP
  port: 80
  targetPort: 8000

ingress:
  enabled: true
  className: nginx  # Use nginx-ingress-controller
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod  # Assumes ClusterIssuer 'letsencrypt-prod' from cert-manager; adjust if different
    nginx.ingress.kubernetes.io/ssl-redirect: "true"  # Redirect HTTP to HTTPS
    # Additional nginx annotations if needed, e.g., nginx.ingress.kubernetes.io/proxy-body-size: "100m" for large responses
  hosts:
    - host: vllm.deweever.bsisandbox.com  # Change to your domain
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: vllm-tls  # Auto-generated by cert-manager
      hosts:
        - vllm.deweever.bsisandbox.com

resources:
  requests:
    cpu: 2
    memory: 8Gi
    nvidia.com/gpu: 1
  limits:
    cpu: 4
    memory: 16Gi
    nvidia.com/gpu: 1

nodeSelector:
  type: g

tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

affinity: {}

vllmConfig:
  model: meta-llama/Llama-2-7b-chat-hf  # Default model, customize as needed
  tensor_parallel_size: 1
  max_model_len: 4096
  gpu_memory_utilization: 0.9
  # Additional vLLM args can be added here if the chart supports

extraEnv: []
# For secrets management, add to extraEnv or chart specific (e.g., HF_TOKEN from secret)

persistence:
  enabled: false  # Enable if you want persistent storage for models
  storageClass: ""
  accessMode: ReadWriteOnce
  size: 100Gi

podSecurityContext:
  fsGroup: 1000

securityContext:
  allowPrivilegeEscalation: false