# Custom values for NVIDIA GPU Operator Helm chart - Production-like setup for AWS EKS with GPU nodes
# Assumptions: EKS cluster with GPU node group (e.g., g4dn.xlarge, labeled "type": "gpu"); OIDC enabled.
# Prerequisites: EKS cluster running; NVIDIA drivers pre-installed or managed by operator; no toolkit needed.
# Best practices: Disable unnecessary components (toolkit), pin driver version, set resource limits to prevent OOM on GPU nodes.
# For EKS: Use nodeSelector to target GPU nodes; integrate with IRSA if using external secrets for NGC.
# Update chart version in app.yaml when upgrading; check NVIDIA docs for latest driver.version.
# Post-install: Validate with kubectl get pods -n gpu-operator; ensure GPU workloads can schedule on GPU nodes.

# Global settings
global:
  imageRegistry: nvcr.io/nvidia  # Default NVIDIA registry
  imagePullPolicy: IfNotPresent
  # For air-gapped: Set proxy or local registry

# Driver: Install NVIDIA driver on GPU nodes
driver:
  enabled: true
  version: "570.172.08"  # Latest stable as per original; check https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/supported-platforms.html for updates
  image: nvcr.io/nvidia/k8s-device-plugin:0.15.0  # Pin compatible version
  repository: nvcr.io/nvidia/driver
  # RepoConfig: Add if using custom driver repo
  # For EKS: Ensure instance types support this driver (e.g., g4dn)

# Toolkit: Disabled as per original setup
toolkit:
  enabled: false  # Disable if not needed for container toolkit

# Device Plugin: Enable for GPU resource allocation
devicePlugin:
  enabled: true
  # image: nvcr.io/nvidia/k8s-device-plugin:0.15.0
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi
  # Security: Run as non-root
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 2000

# Container Toolkit: Optional, but disabled with toolkit
containerToolkit:
  enabled: false  # Align with toolkit.enabled

# DCGM Exporter: For GPU monitoring
dcgm:
  enabled: true
  env:
    - name: DCGM_EXPORTER_NAMESPACE
      value: gpu-operator
    - name: DCGM_EXPORTER_INSTALL_DIR
      value: /usr/local/nvidia/bin
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi
  nodeSelector:
    type: gpu  # Target GPU nodes from EKS node group labels
  tolerations: []  # Add if GPU nodes have taints

# Node Feature Discovery: To label GPU nodes
nodeFeatureDiscovery:
  enabled: true
  # Cron interval for labeling
  cron:
    schedule: "0 */12 * * *"  # Run every 12 hours
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 100m
      memory: 128Mi
  nodeSelector:
    type: gpu

# Operator Validator: To validate GPU setup
operatorValidator:
  enabled: true
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 100m
      memory: 128Mi

# MIG Manager: If using MIG, enable
migManager:
  enabled: false  # Enable for multi-instance GPU

# Cluster Policy: Default
clusterPolicy:
  defaultRuntime: nvidia  # For CUDA workloads

# Install CRDs
crds:
  enabled: true

# Namespace-specific: Deploy to 'gpu-operator' (handled in app.yaml)
# For EKS Fargate: Not supported for GPU; use EC2 GPU instances only
# nodeSelector: { type: gpu }  # Applied per component where relevant

# Post-sync: Run nvidia-smi on GPU pod to verify; ensure vLLM or other GPU apps can use
# Troubleshooting: Check logs for driver installation; ensure no conflicts with EKS AMI drivers