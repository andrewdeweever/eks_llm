apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: gpu-operator
  namespace: argocd  # Assumes ArgoCD in 'argocd' namespace
  # Labels: e.g., app.kubernetes.io/name: gpu-operator
spec:
  project: default  # Dedicated project recommended for GPU workloads
  source:
    repoURL: https://helm.ngc.nvidia.com/nvidia  # Official NVIDIA Helm repo
    chart: gpu-operator
    targetRevision: v25.3.4  # Latest stable version as of 2025-10-01; pin for production, check NVIDIA docs for updates
    helm:
      releaseName: gpu-operator
      values: |
        global:
          imageRegistry: nvcr.io/nvidia  # Default NVIDIA registry
          imagePullPolicy: IfNotPresent
        driver:
          enabled: true
          version: "570.172.08"
        toolkit:
          enabled: false 
        devicePlugin:
          enabled: true
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 200m
              memory: 256Mi
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
            fsGroup: 2000
        containerToolkit:
          enabled: false
        dcgm:
          enabled: true
          env:
            - name: DCGM_EXPORTER_NAMESPACE
              value: gpu-operator
            - name: DCGM_EXPORTER_INSTALL_DIR
              value: /usr/local/nvidia/bin
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 200m
              memory: 256Mi
          nodeSelector:
            type: gpu
          tolerations: []
        nodeFeatureDiscovery:
          enabled: true
          cron:
            schedule: "0 */12 * * *"  # Run every 12 hours
          resources:
            requests:
              cpu: 50m
              memory: 64Mi
            limits:
              cpu: 100m
              memory: 128Mi
          nodeSelector:
            type: gpu
        operatorValidator:
          enabled: true
          resources:
            requests:
              cpu: 50m
              memory: 64Mi
            limits:
              cpu: 100m
              memory: 128Mi
        clusterPolicy:
          defaultRuntime: nvidia  # For CUDA workloads
        crds:
          enabled: true
  destination:
    server: https://kubernetes.default.svc  # In-cluster Kubernetes API
    namespace: gpu-operator  # Dedicated namespace
  syncPolicy:
    automated:
      prune: true  # Automatically delete resources not in Git
      selfHeal: true  # Automatically sync if out of sync
    syncOptions:
      - CreateNamespace=true  # Create the 'gpu-operator' namespace if it doesn't exist
      - ApplyOutOfSyncOnly=true  # Only apply changes if out of sync
    # Sync waves: Chart handles CRDs and dependencies (driver first)
  # IgnoreDifferences: Add for dynamic GPU resources if needed
  # info: {}  # Optional: Add post-sync hooks for validation
# Assumptions: AWS EKS with GPU node group (e.g., g4dn.xlarge, labeled "type": gpu); OIDC enabled for IRSA if using NGC private images.
# Prerequisites: EKS cluster running; no pre-installed NVIDIA drivers to avoid conflicts.
# Best practices: Disable toolkit if not needed, pin driver version, use nodeSelector for GPU nodes in values.yaml.
# For private Git repo: Configure ArgoCD repo credentials.
# Alternatives: Git-based from https://github.com/NVIDIA/gpu-operator.git, path: charts/gpu-operator